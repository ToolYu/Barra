{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9d44b2",
   "metadata": {},
   "source": [
    "# Barra CNE-6 Factor Calculation Function \n",
    "\n",
    "#### Size, Momentum, Volatility, Liquidity, Value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae3ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import *\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Size-----------------------------------------------------------------------------\n",
    "def get_size_factors(dates, end_date, count, stocks, mv_df):\n",
    "    \"\"\"\n",
    "    Calculates the size factors for given stock pool\n",
    "\n",
    "    Parameters:\n",
    "    - dates (DataFrame): Trading dates\n",
    "    - end_date (str): End date\n",
    "    - count (int): Number of trading dates considered\n",
    "    - stocks (list): List of stock codes\n",
    "    - mv_df (DataFrame): Circulating market cap for all stocks\n",
    "\n",
    "    Return: A DataFrame containing date, code, LNCAP, and MIDCAP columns.\n",
    "    \n",
    "    \"\"\"\n",
    "    def MAD_winsorize(x, multiplier):\n",
    "        \"\"\"\n",
    "        Winsorizes data using Median Absolute Deviation (MAD)\n",
    "\n",
    "        \"\"\" \n",
    "        # Calculate the median and median absolute deviation\n",
    "        x_M, x_MAD = np.nanmedian(x), np.nanmedian(np.abs(x - np.nanmedian(x)))\n",
    "        # Find upper and lower bounds\n",
    "        upper, lower = x_M + multiplier * x_MAD, x_M - multiplier * x_MAD \n",
    "        # Replace outliers with the bounds\n",
    "        x[x > upper], x[x < lower] = upper, lower \n",
    "        return x\n",
    "    \n",
    "    def __reg(df):\n",
    "        \"\"\"\n",
    "        Orthogonalize size factors using weighted regression\n",
    "\n",
    "        \"\"\"\n",
    "        y = df['sub_MIDCAP'].values\n",
    "        \n",
    "        # Construct matrix with an intercept and independent variable\n",
    "        X = np.c_[np.ones(len(y)), df['LNCAP'].values] \n",
    "        \n",
    "        # Construct weight matrix using circulating market cap\n",
    "        W = np.diag(np.sqrt(df['Circulating Market Cap']))\n",
    "        \n",
    "        # Find regression coefficient \n",
    "        beta = np.linalg.pinv(X.T @ W @ X) @ X.T @ W @ y \n",
    "        \n",
    "        # Calculate residuals and apply winsorization           \n",
    "        resi = MAD_winsorize(y - X@beta, multiplier=5)\n",
    "        \n",
    "        # Return the standardized residuals\n",
    "        return pd.Series((resi - np.nanmean(resi)) / np.nanstd(resi), index=df['code'])\n",
    "    \n",
    "    # Determine the index of the  end_date \n",
    "    if pd.to_datetime(end_date) not in dates[0].values: \n",
    "        index_end = dates[0][dates[0] < pd.to_datetime(end_date)].idxmax()\n",
    "    else:\n",
    "        index_end =  dates[dates[0] == pd.to_datetime(end_date)].index.item()  \n",
    "        \n",
    "    # Determine start date\n",
    "    start_date = dates[0].iloc[index_end - count] \n",
    "    \n",
    "    #Filter data with given conditions\n",
    "    datacur = mv_df[(start_date < mv_df['date']) & (mv_df['date'] < end_date) & mv_df['code'].isin(stocks)].dropna()\n",
    "    \n",
    "    # Scale the circulating market cap\n",
    "    datacur['Circulating Market Cap'] /= 1e4  \n",
    "    \n",
    "    # Calculate factor LNCAP\n",
    "    datacur['LNCAP'] = np.log(datacur['Circulating Market Cap'] + 1) \n",
    "    datacur['sub_MIDCAP'] = datacur['LNCAP']**3                \n",
    "    \n",
    "    # Group by date and apply the regression function\n",
    "    MIDCAP = datacur.groupby('date').apply(__reg) \n",
    "    datacur = datacur.merge(MIDCAP.reset_index(name='MIDCAP'))\n",
    "    print('Get size factors success!')\n",
    "    return datacur[['date', 'code', 'LNCAP', 'MIDCAP']]\n",
    "\n",
    "\n",
    "#Momentum-----------------------------------------------------------------------------\n",
    "def get_momentum_factors(dates, end_date, count, stocks, cl,mv_df):\n",
    "    \"\"\"\n",
    "    Calculate momentum factors for given stock data.\n",
    "\n",
    "    Parameters:\n",
    "    - dates: A DataFrame with trading dates.\n",
    "    - end_date: The end date for the analysis.\n",
    "    - count: The count used to determine the range of dates.\n",
    "    - stocks: List of stock codes.\n",
    "    - cl: DataFrame containing stock closing prices.\n",
    "    - mv_df: DataFrame containing circulating market values for stocks.\n",
    "\n",
    "    Returns:\n",
    "    - factor: DataFrame with Short Term Reversal, Seasonality, Industry Momentum, Relative_strength, .\n",
    "    \"\"\"\n",
    "    # Determine the index of the  end_date\n",
    "    if pd.to_datetime(end_date) not in dates[0].values:\n",
    "        index_end = dates[0][dates[0] < pd.to_datetime(end_date)].idxmax()\n",
    "    else:\n",
    "        index_end =  dates[dates[0] == pd.to_datetime(end_date)].index.item() \n",
    "        \n",
    "    # Determine start date\n",
    "    s = dates[0].iloc[index_end - count]\n",
    "    \n",
    "    # Calculate Short Term Reversal\n",
    "    # Determine start date for calculation\n",
    "    s1 = dates[0].iloc[index_end - count - 41]\n",
    "    \n",
    "    # Filter stock prices within the specified dates and stock pool\n",
    "    price1 = cl\n",
    "    price1 = price1[(s1 < price1['date']) & (price1['date'] < end_date)]\n",
    "    price1 = price1[price1['code'].isin(stocks)]\n",
    "    \n",
    "    # Find return and change format\n",
    "    price1['ret'] = price1['close'] / price1['pre_close'] - 1 \n",
    "    ret = pd.pivot_table(price1, values='ret', index='date', columns='code')\n",
    "    r_n = ret.rolling(21).mean().dropna(how='all')\n",
    "    \n",
    "    # Apply exponential weighting to returns\n",
    "    W = get_exponent_weight(window=21, half_life=5)\n",
    "    STREV = []\n",
    "    for i in range(len(r_n)-20):\n",
    "        # Calculate log returns\n",
    "        tmp = np.log(1 + r_n.iloc[i:i+21, :].copy())\n",
    "        # Calculate weighted sum of log returns\n",
    "        tmp = pd.Series(np.sum(W.reshape(-1, 1)*tmp.values, axis=0), name=tmp.index[-1], index=tmp.columns)\n",
    "        STREV.append(tmp)\n",
    "    STREV = pd.concat(STREV, axis=1).T\n",
    "    STREV.index.name = 'date'\n",
    "    STREV = pd.melt(STREV.reset_index(), id_vars='date', value_name='Short_Term_reversal').dropna()\n",
    "\n",
    "    # Calculate Seasonality Factor\n",
    "    trade_date = dates[(s < dates[0]) & (dates[0] < end_date)]\n",
    "    season = {}\n",
    "    for td in trade_date[0]:\n",
    "        r_y = []\n",
    "        for i in range(1, 4):\n",
    "            # Find the closest date that is greater than the calculated s_\n",
    "            s_ = td - pd.Timedelta(days=365*i)\n",
    "            # Calculate e_ based on s_\n",
    "            if s_ not in dates[0].values:\n",
    "                closest_date_index = dates[0][dates[0] > pd.to_datetime(s_)].idxmin()\n",
    "                s_ = dates[0].iloc[closest_date_index]  \n",
    "                e_ = dates[0].iloc[closest_date_index + 21]\n",
    "            else:\n",
    "                s_p = dates[dates[0] == s_].index\n",
    "                e_ = dates[0].iloc[s_p + 21].values[0]\n",
    "            \n",
    "            p_ = cl[['date','code','close']]\n",
    "            # Filter stock prices within the specified date range\n",
    "            p_ = p_[(s_ < p_['date']) & (p_['date'] < e_)]\n",
    "            p_ = p_[p_['code'].isin(stocks)]\n",
    "            # Create a pivot table of stock prices and calculate returns\n",
    "            p_ = pd.pivot_table(p_, index='date', columns='code', values='close').ffill()\n",
    "            r_y.append(p_.iloc[-1, :] / p_.iloc[0, :] - 1)\n",
    "        season[pd.to_datetime(td)] = pd.concat(r_y, axis=1).mean(axis=1)\n",
    "        \n",
    "    season = pd.DataFrame(season).T\n",
    "    season.index.name = 'date'\n",
    "    season = pd.melt(season.reset_index(), id_vars='date', value_name='Seasonality')\n",
    "    factor = pd.merge(STREV, season)\n",
    "    \n",
    "    # Calculate Industry Momentum\n",
    "    # find start date for calculation\n",
    "    s2 = dates[0].iloc[index_end - count - 126]\n",
    "    \n",
    "    # filter prices within specified dates and stock pools\n",
    "    price2 = cl\n",
    "    price2 = price2[(s2 < price2['date']) & (price2['date'] < end_date)]\n",
    "    price2 = price2[price2['code'].isin(stocks)]\n",
    "    \n",
    "    # find return\n",
    "    price2['ret'] = price2['close'] / price2['pre_close'] - 1 \n",
    "    ret = pd.pivot_table(price2, index='date', columns='code', values='ret')\n",
    "    \n",
    "    # Apply exponential weighting to returns\n",
    "    W = get_exponent_weight(window=126, half_life=21)\n",
    "    \n",
    "    RS = {}\n",
    "    for i in range(len(ret)-125):\n",
    "        # Extract a 126-day window of returns for analysis\n",
    "        tmp = ret.iloc[i:i+126, :].copy()\n",
    "        # Filter out stocks with more than 10% missing data and fill NaN values with 0\n",
    "        tmp = tmp.loc[:, tmp.isnull().sum(axis=0) / 252 <= 0.1].fillna(0.)\n",
    "        # Calculate log returns\n",
    "        tmp = np.log(1 + tmp)\n",
    "        # Calculate RS values\n",
    "        RS[tmp.index[-1]] = pd.Series(np.sum(W.reshape(-1, 1)*tmp.values, axis=0), index=tmp.columns)\n",
    "    RS = pd.DataFrame(RS).T\n",
    "    RS.index.name = 'date'\n",
    "    RS = pd.melt(RS.reset_index(), id_vars='date', value_name='RS').dropna().reset_index(drop=True)\n",
    "    \n",
    "    #filter circulating market cap within specified dates and stock pools\n",
    "    val = mv_df\n",
    "    val = val[(s < val['date']) & (val['date'] < end_date)]\n",
    "    val = val[val['code'].isin(stocks)]\n",
    "    \n",
    "    RS = pd.merge(RS, val)\n",
    "    # Extract the month and year from the 'date' column and store it as 'mon'\n",
    "    RS['mon'] = RS['date'].apply(lambda x: x.strftime(\"%Y-%m-01\"))\n",
    "    \n",
    "    # Extract the month and year from the 'date' column and store it as 'mon'\n",
    "    RS['c'] = np.sqrt(RS['Circulating Market Cap'])\n",
    "\n",
    "    def __industry_RS(x):\n",
    "        ind_RS = x.groupby('industry').apply(\n",
    "            lambda y: y['RS'].dot(y['c']) / y['c'].sum()\n",
    "        )\n",
    "        # Group data by industry and calculate industry-specific RS\n",
    "        ind_RS.name = 'ind_RS'\n",
    "        ind_RS = ind_RS.reset_index()\n",
    "        x = pd.merge(x, ind_RS)\n",
    "        # Calculate the Industry Momentum\n",
    "        x['Industry_Momentum'] = x['ind_RS'] - x['RS']\n",
    "        return x[['code', 'Industry_Momentum']].set_index('code')\n",
    "    \n",
    "    INDMOM = []\n",
    "    \n",
    "    # filter industry data\n",
    "    ind = industry_long[industry_long['date'] == pd.to_datetime('2022-12-30')]\n",
    "    ind = ind[['code','industry']]\n",
    "    # Group the RS DataFrame by month ('mon')\n",
    "    for m, tmp_RS in RS.groupby('mon'):\n",
    "        tmp_RS = pd.merge(tmp_RS, ind)\n",
    "        print(tmp_RS)\n",
    "        # Calculate Industry Momentum\n",
    "        INDMOM.append(tmp_RS.groupby('date').apply(__industry_RS).reset_index())\n",
    "    INDMOM = pd.concat(INDMOM).reset_index(drop=True)\n",
    "#     print(INDMOM)\n",
    "    factor = factor.merge(INDMOM)\n",
    "\n",
    "    # Calculate Relative Strength\n",
    "    s3 = dates[0].iloc[index_end - count - 262]\n",
    "    W = get_exponent_weight(window=252, half_life=126)\n",
    "    \n",
    "    # filter price and find return \n",
    "    price3 = cl_long\n",
    "    price3 = price3[(s3 < price3['date']) & (price3['date'] < end_date)]\n",
    "    price3 = price3[price3['code'].isin(stocks)]\n",
    "    price3['ret'] = np.log(price3['close']) - np.log(price3['pre_close'])\n",
    "    ret = pd.pivot_table(price3, index='date', columns='code', values='ret')\n",
    "\n",
    "    Relative_strength = {}\n",
    "    for i in range(len(ret) - 251):\n",
    "        # Extract a 252-day window of returns for analysis\n",
    "        tmp = ret.iloc[i:i+252, :]\n",
    "        # Filter out stocks with more than 10% missing data\n",
    "        tmp = tmp.loc[:, tmp.isnull().sum(axis=0) / 252 <= 0.1].fillna(0.)\n",
    "        # Calculate the weighted sum of returns for the selected window\n",
    "        np.sum(W.reshape(-1, 1)*tmp.values, axis=0)\n",
    "        # Calculate Relative Strength\n",
    "        Relative_strength[tmp.index[-1]] = pd.Series(np.sum(W.reshape(-1, 1)*tmp.values, axis=0), index=tmp.columns)\n",
    "    Relative_strength = pd.DataFrame(Relative_strength).T\n",
    "    Relative_strength.index.name = 'date'\n",
    "    Relative_strength = Relative_strength.rolling(11).mean().dropna(how='all')\n",
    "    Relative_strength = pd.melt(Relative_strength.reset_index(), id_vars='date', value_name='Relative_strength').dropna().reset_index(drop=True)\n",
    "    #print(Relative_strength)\n",
    "    factor = factor.merge(Relative_strength)\n",
    "    clear_output()\n",
    "    print ('Get size factors success!')\n",
    "    return factor\n",
    "\n",
    "#Volatility-----------------------------------------------------------------------------\n",
    "def get_volatility_factors(dates, end_date, count, window, half_life ,stocks, cl_long,dqmv_long, wind_price):\n",
    "    \"\"\"\n",
    "    Calculate volatility factors for given stock data.\n",
    "\n",
    "    Parameters:\n",
    "    - dates: A DataFrame with trading dates.\n",
    "    - end_date: The end date for the analysis.\n",
    "    - count: The count used to determine the range of dates.\n",
    "    - window: The size of the rolling window\n",
    "    - half_life: The half-life for exponential weighting\n",
    "    - stocks: List of stock codes.\n",
    "    - cl: DataFrame containing stock closing prices.\n",
    "    - dqmv_df: DataFrame containing circulating market values for stocks.\n",
    "\n",
    "    Returns:\n",
    "    - factor: DataFrame with Beta, Hist sigma, Daily std, Cumulative range\n",
    "    \"\"\"\n",
    "    \n",
    "    # find index of end date\n",
    "    if pd.to_datetime(end_date) not in dates[0].values:\n",
    "        index_end = dates[0][dates[0] < pd.to_datetime(end_date)].idxmax()\n",
    "    else:\n",
    "        index_end =  dates[dates[0] == pd.to_datetime(end_date)].index.item() \n",
    "    \n",
    "    # find start date\n",
    "    s = dates[0].iloc[index_end - count]\n",
    "    start_date = dates[0].iloc[index_end - count - window]\n",
    "    \n",
    "    # filter stock prices\n",
    "    price = cl_long\n",
    "    price = price[(start_date < price['date']) & (price['date'] < end_date)]\n",
    "    price = price[price['code'].isin(stocks)]\n",
    "    \n",
    "    # filter Wind All China Index prices \n",
    "    wind = wind_price[(start_date < wind_price['date']) & (wind_price['date'] < end_date)].copy()\n",
    "    wind.loc[:, 'code'] = '881001'\n",
    "    price = pd.concat([price, wind]).reset_index(drop=True)\n",
    "    price = price.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # find return\n",
    "    price['ret'] = price['close'] / price['pre_close'] - 1\n",
    "    ret = pd.pivot_table(price, values='ret', index='date', columns='code')\n",
    "    \n",
    "    # Calculate Lambda (decay factor) and initialize weights 'W' for exponential weighting\n",
    "    L, Lambda = 0.5**(1/half_life), 0.5**(1/half_life)\n",
    "    W = []\n",
    "    for i in range(window):\n",
    "        W.append(Lambda)\n",
    "        Lambda *= L\n",
    "    W = W[::-1]\n",
    "    \n",
    "    # Calculate Lambda (decay factor) and initialize weights 'W' for exponential weighting\n",
    "    beta, hist_sigma = [], []\n",
    "    for i in range(len(ret)-window+1):\n",
    "        tmp = ret.iloc[i:i+window, :].copy()\n",
    "        W_full = np.diag(W)\n",
    "        Y_full = tmp.dropna(axis=1).drop(columns='881001')\n",
    "        idx_full, Y_full = Y_full.columns, Y_full.values\n",
    "        X_full = np.c_[np.ones((window, 1)), tmp.loc[:, '881001'].values]\n",
    "        \n",
    "        # Calculate BETA and hist_sigma for the full dataset\n",
    "        beta_full = np.linalg.pinv(X_full.T@W_full@X_full)@X_full.T@W_full@Y_full\n",
    "        hist_sigma_full = pd.Series(np.std(Y_full - X_full@beta_full, axis=0), index=idx_full, name=tmp.index[-1])\n",
    "        beta_full = pd.Series(beta_full[1], index=idx_full, name=tmp.index[-1])\n",
    "        \n",
    "        beta_lack, hist_sigma_lack = {}, {}\n",
    "        for c in set(tmp.columns) - set(idx_full) - set('881001'):\n",
    "            tmp_ = tmp.loc[:, [c, '881001']].copy()\n",
    "            tmp_.loc[:, 'W'] = W\n",
    "            tmp_ = tmp_.dropna()\n",
    "            W_lack = np.diag(tmp_['W'])\n",
    "            if len(tmp_) < half_life:\n",
    "                continue\n",
    "            X_lack = np.c_[np.ones(len(tmp_)), tmp_['881001'].values]\n",
    "            Y_lack = tmp_[c].values\n",
    "            \n",
    "            # Calculate BETA and hist_sigma for stocks with incomplete data\n",
    "            beta_tmp = np.linalg.pinv(X_lack.T@W_lack@X_lack)@X_lack.T@W_lack@Y_lack\n",
    "            hist_sigma_lack[c] = np.std(Y_lack - X_lack@beta_tmp)\n",
    "            beta_lack[c] = beta_tmp[1]\n",
    "        beta_lack = pd.Series(beta_lack, name=tmp.index[-1])\n",
    "        hist_sigma_lack = pd.Series(hist_sigma_lack, name=tmp.index[-1])\n",
    "        beta.append(pd.concat([beta_full, beta_lack]).sort_index())\n",
    "        hist_sigma.append(pd.concat([hist_sigma_full, hist_sigma_lack]).sort_index())\n",
    "    beta = pd.concat(beta, axis=1).T\n",
    "    beta = pd.melt(beta.reset_index(), id_vars='index').dropna()\n",
    "    beta.columns = ['date', 'code', 'BETA']\n",
    "    hist_sigma = pd.concat(hist_sigma, axis=1).T\n",
    "    hist_sigma = pd.melt(hist_sigma.reset_index(), id_vars='index').dropna()\n",
    "    hist_sigma.columns = ['date', 'code', 'Hist_sigma']\n",
    "    factor = pd.merge(beta, hist_sigma)\n",
    "    \n",
    "    # Calculate the Exponentially Weighted Moving Average (EWMA) of daily standard deviation (Daily_std)\n",
    "    # Initialize the standard deviation and variance\n",
    "    init_std = ret.std(axis=0)\n",
    "    L = 0.5**(1/42)\n",
    "    init_var = ret.var(axis=0)\n",
    "    tmp = init_var.copy()\n",
    "    daily_std = {}\n",
    "    \n",
    "    # calculate daily_std\n",
    "    for t, k in ret.iterrows():\n",
    "        tmp = tmp*L + k**2*(1-L)\n",
    "        daily_std[t] = np.sqrt(tmp)\n",
    "        tmp = tmp.fillna(init_var)\n",
    "    daily_std = pd.DataFrame(daily_std).T\n",
    "    daily_std.index.name = 'date'\n",
    "    daily_std = daily_std.loc[s:end_date, :]\n",
    "    daily_std = pd.melt(daily_std.reset_index(), id_vars='date', value_name='Daily_std').dropna()\n",
    "\n",
    "    factor = factor.merge(daily_std)\n",
    "\n",
    "    close = pd.pivot_table(price, values='close', index='date', columns='code').fillna(method='ffill', limit=10)\n",
    "\n",
    "    pre_close = pd.pivot_table(price, values='pre_close', index='date', columns='code').fillna(method='ffill', limit=10)\n",
    "    idx = close.index\n",
    "    CMRA = {}\n",
    "    \n",
    "    # Calculate the Cumulative Moving Range Average (CMRA)\n",
    "    for i in range(len(close)-window+1):\n",
    "        close_ = close.iloc[i:i+window, :]\n",
    "        pre_close_ = pre_close.iloc[i, :]\n",
    "        pre_close_.name = pre_close_.name - pd.Timedelta(days=1)\n",
    "        pre_close_1 = pd.DataFrame([pre_close_.values], columns=pre_close_.index, index=[pre_close_.name])\n",
    "        close_ = pd.concat([close_, pre_close_1], axis=1)\n",
    "        close_ = close_.sort_index().iloc[list(range(0, 253, 21)), :]\n",
    "        r_tau = close_.pct_change().dropna(how='all')\n",
    "        Z_T = np.log(r_tau+1).cumsum(axis=0)\n",
    "        CMRA[idx[i+window-1]] = Z_T.max(axis=0) - Z_T.min(axis=0)\n",
    "    CMRA = pd.DataFrame(CMRA).T\n",
    "    CMRA.index.name = 'date'\n",
    "    CMRA = pd.melt(CMRA.reset_index(), id_vars='date', value_name='Cumulative_range').dropna()\n",
    "    factor = factor.merge(CMRA)\n",
    "    # Remove rows where 'Cumulative_range' is equal to 0\n",
    "    factor = factor[factor['Cumulative_range'] != 0] \n",
    "    \n",
    "    # Remove rows of Wind All China Index\n",
    "    factor = factor[factor['code'] != '881001']\n",
    "\n",
    "    clear_output()\n",
    "    print ('Get size factors success!')\n",
    "    return factor.sort_values(by=['date', 'code'])\n",
    "\n",
    "#Liquidity-----------------------------------------------------------------------------\n",
    "def get_liquidity_factors(end_date, count, stocks,turn):\n",
    "    \"\"\"\n",
    "    Calculate liquidity factors for given stock data.\n",
    "\n",
    "    Parameters:\n",
    "    - dates: A DataFrame with trading dates.\n",
    "    - end_date: The end date for the analysis.\n",
    "    - count: The count used to determine the range of dates.\n",
    "    - turn: A dataframe with turnover-ratio \n",
    "\n",
    "    Returns:\n",
    "    - factor: DataFrame with Monthly share turnover, Quarterly share turnover, Annual share turnover\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the index of end_date\n",
    "    if pd.to_datetime(end_date) not in dates[0].values:\n",
    "        index_end = dates[0][dates[0] < pd.to_datetime(end_date)].idxmax()\n",
    "    else:\n",
    "        index_end =  dates[dates[0] == pd.to_datetime(end_date)].index.item() \n",
    "    \n",
    "    # find start date\n",
    "    s = dates[0].iloc[index_end - count]\n",
    "    \n",
    "    # find start date of data for calculation\n",
    "    start_date = dates[0].iloc[index_end - count - 252]\n",
    "    \n",
    "    # filter turnover ratio data\n",
    "    tmp = turn_long\n",
    "    tmp = tmp[(start_date < tmp['date']) & (tmp['date'] < end_date)]\n",
    "    tmp = tmp[tmp['code'].isin(stocks)]\n",
    "    tmp['turnover_ratio'] /= 100.\n",
    "    tmp = pd.pivot_table(tmp, index='date', columns='code', values='turnover_ratio')\n",
    "\n",
    "    # Calculate monthly, quarterly, and annual share turnover factor\n",
    "    monthly_share_turnover = np.log(tmp.rolling(21).sum()+1e-10)\n",
    "    idx = list(range(20, 252, 21))\n",
    "    quarterly_share_turnover, annual_share_turnover = {}, {}\n",
    "\n",
    "    for i in range(len(tmp) - 251):\n",
    "        # get t for calculation\n",
    "        t = tmp.index[i+251]\n",
    "        # taking the exponential of the monthly share turnover values for 252 days\n",
    "        mst = np.exp(monthly_share_turnover.iloc[i:i+252, :].iloc[idx, :])\n",
    "        # taking the logarithm of the mean of the last 3 monthly values\n",
    "        quarterly_share_turnover[t] = np.log(mst.iloc[-3:, :].mean(axis=0))\n",
    "        # taking the logarithm of the mean of a year\n",
    "        annual_share_turnover[t] = np.log(mst.mean(axis=0))\n",
    "    quarterly_share_turnover = pd.DataFrame(quarterly_share_turnover).T\n",
    "    annual_share_turnover = pd.DataFrame(annual_share_turnover).T\n",
    "    quarterly_share_turnover.index.name = 'date'\n",
    "    annual_share_turnover.index.name = 'date'\n",
    "    monthly_share_turnover = monthly_share_turnover.loc[s:end_date, :]\n",
    "    monthly_share_turnover = pd.melt(monthly_share_turnover.reset_index(), id_vars='date', value_name='Monthly_share_turnover').dropna()\n",
    "    quarterly_share_turnover = pd.melt(quarterly_share_turnover.reset_index(), id_vars='date', value_name='Quarterly_share_turnover').dropna()\n",
    "    annual_share_turnover = pd.melt(annual_share_turnover.reset_index(), id_vars='date', value_name='Annual_share_turnover').dropna()\n",
    "    \n",
    "    factor = monthly_share_turnover.merge(quarterly_share_turnover).merge(annual_share_turnover)\n",
    "\n",
    "    # Calculate annualized traded value ratio\n",
    "    window, half_life = 252, 63\n",
    "    L, Lambda = 0.5**(1/half_life), 0.5**(1/half_life)\n",
    "    W = []\n",
    "    for i in range(window):\n",
    "        W.append(Lambda)\n",
    "        Lambda *= L\n",
    "    W = np.array(W[::-1])/np.mean(W)\n",
    "    \n",
    "    annualized_traded_value_ratio = []\n",
    "\n",
    "    for i in range(len(tmp) - 251):\n",
    "        tmp_ = tmp.iloc[i:i + 252, :].copy()\n",
    "        values = tmp_.values * W.reshape(-1, 1)\n",
    "\n",
    "        # Replace NaN values with zeros\n",
    "        values = np.nan_to_num(values)\n",
    "\n",
    "        mean_values = np.nanmean(values, axis=0)\n",
    "        annualized_traded_value_ratio.append(\n",
    "            pd.Series(mean_values, index=tmp.columns, name=tmp_.index[-1])\n",
    "        )\n",
    "    annualized_traded_value_ratio = pd.concat(annualized_traded_value_ratio, axis=1).T * window\n",
    "    annualized_traded_value_ratio.index.name = 'date'\n",
    "    annualized_traded_value_ratio = pd.melt(annualized_traded_value_ratio.reset_index(), id_vars='date', value_name='Annualized_traded_value_ratio').dropna()\n",
    "    factor = factor.merge(annualized_traded_value_ratio)\n",
    "    return factor\n",
    "\n",
    "\n",
    "#Value-----------------------------------------------------------------------------\n",
    "def get_value_factors(end_date, count,stocks, ratio, cl,wind):\n",
    "    \"\"\"\n",
    "    Calculate value factors for given stock data.\n",
    "\n",
    "    Parameters:\n",
    "    - end_date: The end date for the analysis.\n",
    "    - count: The count used to determine the range of dates.\n",
    "    - stocks: The list of stock codes\n",
    "    - ratio: Dataframe of pb, pe and pcf ratio\n",
    "    - cl - Dataframe of close price\n",
    "    - wind - Dataframe of Wind All China Index prices\n",
    "\n",
    "    Returns:\n",
    "    - factor: DataFrame with Book to price, EP ratio, Cash earning to price, Long term relative strength, Long term historical alpha\n",
    "    \"\"\"\n",
    "    # find end date index\n",
    "    if pd.to_datetime(end_date) not in dates[0].values:\n",
    "        index_end = dates[0][dates[0] < pd.to_datetime(end_date)].idxmax()\n",
    "    else:\n",
    "        index_end =  dates[dates[0] == pd.to_datetime(end_date)].index.item() \n",
    "    \n",
    "    # find start date\n",
    "    s = dates[0].iloc[index_end - count]\n",
    "    \n",
    "    # filter all ratio values\n",
    "    val = ratio\n",
    "    val = val[val['code'].isin(stocks)]\n",
    "    val = val[(s < val['date']) & (val['date'] < pd.to_datetime(end_date))]\n",
    "    \n",
    "    # Calculate Book to Price, Earnings to Price, and Cash Earnings to Price\n",
    "    val['Book_to_price'] = 1 / val['pb_ratio']\n",
    "    val['Earning_to_price'] = 1 / val['pe_ratio']\n",
    "    val['Cash_earning_to_price'] = 1 / val['pcf_ratio']\n",
    "    factor = val[['code', 'date', 'Book_to_price', 'Earning_to_price', 'Cash_earning_to_price']]\n",
    "    \n",
    "    # Define a function to calculate cumulative mean\n",
    "    def __cummean(x):\n",
    "        f_ = []\n",
    "        def __sub_cummean(y, f_):\n",
    "            f_ += y['np'].values.tolist()\n",
    "            if len(f_)<5:\n",
    "                return np.nan\n",
    "            return np.nanmean(f_)\n",
    "        np_std = x.groupby('time').apply(lambda z: __sub_cummean(z, f_))\n",
    "        np_std.name = 'np_mean'\n",
    "        return np_std.dropna()\n",
    "\n",
    "    window = 750\n",
    "    half_life = 260\n",
    "    \n",
    "    # Calculate Long-term Relative Strength\n",
    "    s3 = dates[0].iloc[index_end - count - 750 - 12]\n",
    "    W = get_exponent_weight(window=window, half_life=260)\n",
    "    price = cl\n",
    "    price = price[(s3 < price['date']) & (price['date'] < end_date)]\n",
    "    price = price[price['code'].isin(stocks)]\n",
    "    price['ret'] = np.log(price['close']) - np.log(price['pre_close'])\n",
    "    ret = pd.pivot_table(price, index='date', columns='code', values='ret')\n",
    "\n",
    "    Relative_strength = {}\n",
    "    for i in range(len(ret) - window-1):\n",
    "        tmp = ret.iloc[i:i+window, :]\n",
    "        tmp = tmp.loc[:, tmp.isnull().sum(axis=0) / window <= 0.1].fillna(0.)\n",
    "        np.sum(W.reshape(-1, 1)*tmp.values, axis=0)\n",
    "        Relative_strength[tmp.index[-1]] = pd.Series(np.sum(W.reshape(-1, 1)*tmp.values, axis=0), index=tmp.columns)\n",
    "    Relative_strength = pd.DataFrame(Relative_strength).T\n",
    "    Relative_strength.index.name = 'date'\n",
    "    Relative_strength = Relative_strength.rolling(11).mean().dropna(how='all').mul(-1)\n",
    "    Relative_strength = pd.melt(Relative_strength.reset_index(), id_vars='date', value_name='Longterm_Relative_strength')\\\n",
    "        .dropna().reset_index(drop=True)\n",
    "\n",
    "    # Calculate Long-term Alpha (similar to volatility beta cauculation)\n",
    "    price = cl\n",
    "    price = price[(s3 < price['date']) & (price['date'] < end_date)]\n",
    "    price = price[price['code'].isin(stocks)]\n",
    "    wind = wind_price[(s3 < wind_price['date']) & (wind_price['date'] < end_date)].copy()\n",
    "    wind.loc[:, 'code'] = '881001'\n",
    "    price = pd.concat([price, wind]).reset_index(drop=True)\n",
    "    price = price.drop('Unnamed: 0', axis=1)\n",
    "    price['ret'] = price['close'] / price['pre_close'] - 1\n",
    "    ret = pd.pivot_table(price, values='ret', index='date', columns='code')\n",
    "    W = get_exponent_weight(window=window, half_life=half_life)\n",
    "    \n",
    "    alpha = []\n",
    "    for i in range(len(ret)-window+1):\n",
    "        tmp = ret.iloc[i:i+window, :].copy()#\n",
    "        W_full = np.diag(W)#\n",
    "        Y_full = tmp.dropna(axis=1).drop(columns='881001')#\n",
    "\n",
    "        idx_full, Y_full = Y_full.columns, Y_full.values#\n",
    "        X_full = np.c_[np.ones((window, 1)), tmp.loc[:, '881001'].values]#\n",
    "\n",
    "        alpha_full = np.linalg.pinv(X_full.T@W_full@X_full)@X_full.T@W_full@Y_full\n",
    "        alpha_full = pd.Series(alpha_full[1], index=idx_full, name=tmp.index[-1])\n",
    "\n",
    "        alpha_lack = {}\n",
    "        for c in set(tmp.columns) - set(idx_full) - set('881001'):\n",
    "            tmp_ = tmp.loc[:, [c, '881001']].copy()\n",
    "            tmp_.loc[:, 'W'] = W\n",
    "            tmp_ = tmp_.dropna()\n",
    "            W_lack = np.diag(tmp_['W'])\n",
    "            if len(tmp_) < half_life:\n",
    "                continue\n",
    "            X_lack = np.c_[np.ones(len(tmp_)), tmp_['881001'].values]\n",
    "            Y_lack = tmp_[c].values\n",
    "            alpha_tmp = np.linalg.pinv(X_lack.T@W_lack@X_lack)@X_lack.T@W_lack@Y_lack\n",
    "            alpha_lack[c] = alpha_tmp[1]\n",
    "        alpha_lack = pd.Series(alpha_lack, name=tmp.index[-1])\n",
    "        alpha.append(pd.concat([alpha_full, alpha_lack]).sort_index())\n",
    "        \n",
    "    alpha = pd.concat(alpha, axis=1).T\n",
    "    alpha = pd.melt(alpha.reset_index(), id_vars='index').dropna()\n",
    "    alpha.columns = ['date', 'code', 'alpha']\n",
    "    alpha = alpha[alpha['code'] != '881001']\n",
    "    factor = pd.merge(factor,Relative_strength, on = ['code','date'], how = 'outer' )\n",
    "    factor = pd.merge(factor, alpha, how = 'outer')\n",
    "    factor = factor[factor['code'] != '881001'].reset_index()\n",
    "    return factor\n",
    "\n",
    "def get_exponent_weight(window, half_life, is_standardize=True):\n",
    "    # Calculate the decay factors L and Lambda.\n",
    "    L, Lambda = 0.5**(1/half_life), 0.5**(1/half_life)\n",
    "    W = []\n",
    "    # Loop through the specified window size to calculate exponential weights\n",
    "    for i in range(window):\n",
    "        W.append(Lambda)                                                         \n",
    "        Lambda *= L # Update Lambda's value\n",
    "    W = np.array(W[::-1])\n",
    "    if is_standardize:\n",
    "        W /= np.sum(W) # Normalize the weights\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ce958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
